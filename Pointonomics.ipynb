import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import truncnorm

# Define the tasks data with correct interpretation for percentages and numerical values
tasks = {
    'Task Name': [
        'Mint your Reputation NFTs', 'Level-up your Reputation NFTs',
        'Mint on all chains', 'Level-up on all chains', 'Member referral', 
        'Community referral', 'Profil, step 1', 'Profil, step 2', 'Profil, step 3', 
        'Profil, step 4', 'Profil, step 5', 'Twitter follow', 'Twitter PP', 
        'Twitter tweets', 'Resistance member', 'Discord role - Points level 1', 
        'Discord role - Points level 2', 'Discord role - Points level 3', 
        'Product testing', 'Quizz'
    ],
    'Score': [50, 50, 1000, 1000, 100, 1000, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 100, 20],
    'Average User': [
        0.94, 20, 0.90, 20, 3, 1, 0.97, 0.94, 0.88, 0.80, 0.70, 
        0.50, 0.05, 2, 0.005, 0.97, 0.88, 0.70, 0.05, 20
    ],
    'Std Dev': [
        0.30, 7, 0.30, 7, 1000, 1, 0.10, 0.20, 0.25, 0.30, 0.40, 
        0.50, 0.30, 5, 0.02, 0.10, 0.25, 0.40, 0.07, 7
    ],
    'Is Percentage': [
        True, False, True, False, False, False, True, True, True, True, True, 
        True, True, False, True, True, True, True, True, False
    ]
}

# Create a DataFrame
df_tasks = pd.DataFrame(tasks)

# Function to generate truncated normal distribution
def get_truncated_normal(mean, sd, low, upp, size, is_percentage):
    if sd == 0:
        return np.full(size, mean)
    distribution = truncnorm(
        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd).rvs(size)
    if is_percentage:
        distribution = np.clip(distribution, 0, 1)  # Ensure percentage values are within [0, 1]
    return distribution

# Simulate distributions based on average user completion for each task
np.random.seed(42)
total_users = 200000  # Reduced number of users for the simulation

# Generate mock scores based on truncated normal distribution around the average for each task
task_distributions = {}
for index, row in df_tasks.iterrows():
    task_name = row['Task Name']
    mean = row['Average User']
    std_dev = row['Std Dev']
    is_percentage = row['Is Percentage']
    upper_bound = np.inf
    task_distributions[task_name] = get_truncated_normal(mean, std_dev, 0, upper_bound, total_users, is_percentage)

# Generate instantiation days for each user with wider dispersion
instantiation_days = np.random.normal(loc=182.5, scale=60, size=total_users).astype(int)
instantiation_days = np.clip(instantiation_days, 0, 364)  # Ensure days are within the year

# Generate scores for each user with steps and instantiation over 365 days
user_scores = np.zeros((total_users, 365 + 365))  # Extra 365 days for late instantiations
for index, row in df_tasks.iterrows():
    task_name = row['Task Name']
    score = row['Score']
    task_completion = task_distributions[task_name]
    
    for i in range(total_users):
        start_day = instantiation_days[i]
        task_days = np.sort(np.random.choice(range(start_day, start_day + 365), size=int(task_completion[i]), replace=True))
        for day in task_days:
            user_scores[i, day] += score

# Implementing erosion for inactivity periods longer than 15 days
inactivity_threshold = 15  # Number of days of inactivity to trigger penalty
penalty_points = 0  # Fixed penalty for inactivity

# Calculate cumulative scores with steps and penalty for inactivity
cumulative_scores_matrix = np.zeros((total_users, 365 + 365))
for i in range(total_users):
    cumulative_score = 0
    last_active_day = instantiation_days[i]
    consecutive_inactivity_periods = 0  # Count consecutive inactivity periods
    for day in range(instantiation_days[i], 365 + instantiation_days[i]):
        if user_scores[i, day] > 0:
            cumulative_score += user_scores[i, day]
            last_active_day = day
            consecutive_inactivity_periods = 0  # Reset inactivity period count
        else:
            if day - last_active_day > inactivity_threshold:
                consecutive_inactivity_periods += 1
                cumulative_score -= penalty_points * consecutive_inactivity_periods
                cumulative_score = max(cumulative_score, 0)  # Ensure score doesn't go negative
                last_active_day = day
        cumulative_scores_matrix[i, day] = cumulative_score

# Function to apply promotion and demotion based on scores
def apply_promotion_and_demotion_dynamic(cumulative_scores_matrix, period=7):
    num_users, num_days = cumulative_scores_matrix.shape
    num_periods = num_days // period
    
    # Initialize user groups
    user_groups = np.zeros((num_periods, num_users), dtype=int)
    
    for period_index in range(num_periods):
        start_day = period_index * period
        end_day = min(start_day + period, num_days)
        period_scores = cumulative_scores_matrix[:, end_day - 1]
        
        # Rank users by their scores in the current period
        ranked_users = np.argsort(period_scores)[::-1]
        top_1_percent = int(np.ceil(0.01 * num_users))
        top_3_percent = int(np.ceil(0.03 * num_users))
        top_7_percent = int(np.ceil(0.07 * num_users))
        top_14_percent = int(np.ceil(0.14 * num_users))
        top_25_percent = int(np.ceil(0.25 * num_users))
        top_40_percent = int(np.ceil(0.40 * num_users))
        top_65_percent = int(np.ceil(0.65 * num_users))
        
        user_groups[period_index, ranked_users[:top_1_percent]] = 0  # Rhodium: Top 1%
        user_groups[period_index, ranked_users[top_1_percent:top_3_percent]] = 1  # Iridium: Next 2%
        user_groups[period_index, ranked_users[top_3_percent:top_7_percent]] = 2  # Gold: Next 4%
        user_groups[period_index, ranked_users[top_7_percent:top_14_percent]] = 3  # Platinum: Next 7%
        user_groups[period_index, ranked_users[top_14_percent:top_25_percent]] = 4  # Ruthenium: Next 11%
        user_groups[period_index, ranked_users[top_25_percent:top_40_percent]] = 5  # Palladium: Next 15%
        user_groups[period_index, ranked_users[top_40_percent:top_65_percent]] = 6  # Silver: Next 25%
        user_groups[period_index, ranked_users[top_65_percent:]] = 7  # Osmium: Remaining 35%
    
    return user_groups

# Apply promotion and demotion dynamically on a weekly basis
user_groups_over_time = apply_promotion_and_demotion_dynamic(cumulative_scores_matrix[:, :365])

# Weekly tokens to be distributed (total tokens over the year divided by number of weeks)
total_tokens = 12250000
weekly_tokens = total_tokens / 52

# Simulate the corrected weekly token distribution
tokens_distribution_corrected = np.zeros((user_groups_over_time.shape[0], total_users))

# Calculate the corrected token distribution for each period
for period_index in range(user_groups_over_time.shape[0]):
    group = user_groups_over_time[period_index]
    group_sizes = np.bincount(group, minlength=8)
    for g in range(8):
        group_size = group_sizes[g]
        if group_size > 0:
            tokens_distribution_corrected[period_index, group == g] = (weekly_tokens / 8) / group_size

# Calculate the total tokens received by each user over the year (corrected)
total_tokens_per_user_corrected = tokens_distribution_corrected.sum(axis=0)

# Verify if the total number of tokens distributed matches the expected total tokens (corrected)
total_tokens_distributed_corrected = total_tokens_per_user_corrected.sum()

# Check if the total tokens distributed matches the expected total tokens
print(f'Total Tokens Expected: {total_tokens}')
print(f'Total Tokens Distributed: {tokens_distribution_corrected.sum()}')
print(f'Tokens Distribution Match: {total_tokens == total_tokens_distributed_corrected}')

# Plot the distribution of total tokens received per user
plt.figure(figsize=(10, 6))
plt.hist(total_tokens_per_user_corrected, bins=100, color='blue', edgecolor='black')
plt.title('Distribution of Total Tokens Received per User')
plt.xlabel('Total Tokens Received')
plt.ylabel('Number of Users')
plt.grid(False)
plt.show()

# Function to calculate the Lorenz curve
def lorenz_curve(values):
    sorted_values = np.sort(values)
    cumulative_values = np.cumsum(sorted_values)
    lorenz_curve_values = cumulative_values / cumulative_values[-1]
    lorenz_curve_values = np.insert(lorenz_curve_values, 0, 0)
    return lorenz_curve_values

# Function to calculate the Gini coefficient
def gini_coefficient(values):
    sorted_values = np.sort(values)
    n = len(values)
    cumulative_values = np.cumsum(sorted_values)
    relative_mean = cumulative_values / cumulative_values[-1]
    gini_index = (n + 1 - 2 * np.sum(relative_mean)) / n
    return gini_index

# Calculate the Lorenz curve and Gini coefficient for the token distribution
lorenz_elo = lorenz_curve(total_tokens_per_user_corrected)
gini_elo = gini_coefficient(total_tokens_per_user_corrected)

# Plot the Lorenz curve for token distribution
plt.figure(figsize=(10, 6))
plt.plot(np.linspace(0.0, 1.0, lorenz_elo.size), lorenz_elo, label='Lorenz Curve')
plt.plot([0, 1], [0, 1], linestyle='--', color='black', label='Equality Line')
plt.title('Lorenz Curve for Token Distribution based on Elo Ranking')
plt.xlabel('Cumulative Share of Users')
plt.ylabel('Cumulative Share of Tokens')
plt.legend()
plt.grid(False)
plt.show()

# Display Gini coefficient
print(f'Gini Coefficient for Token Distribution based on Elo Ranking: {gini_elo:.4f}')
